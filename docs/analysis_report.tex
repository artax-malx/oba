\documentclass[english, 11pt, a4paper]{article}
\usepackage{amsmath, amssymb,amsthm}
\usepackage{setspace, natbib}
\usepackage{bm}
\usepackage{threeparttable}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[procnames]{listings}
\usepackage{color}
\setlength{\textwidth}{15.5cm} \setlength{\textheight}{22cm}\setlength{\oddsidemargin}{-0.5mm}
\setlength{\parskip}{1ex plus0.5ex minus0.5ex}\setlength{\parindent}{0mm}
\DeclareMathOperator\erfc{erfc}
\usepackage{babel}


\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{language=Python, 
        basicstyle=\ttfamily\small, 
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        procnamekeys={def,class}}


\renewcommand{\bibfont}{\footnotesize}
\setlength{\bibsep}{1pt}

\begin{document}

\baselineskip18pt


\title{Order Book Analysis}

\author{Artagan Malsagov}

\date{\today}


\maketitle
%\newpage 
%\tableofcontents

%\section{Introduction}

\section{Data Description}

The data consist of 5 levels of both sides of the order book, for 5 different days.
Each days spans roughly 10 hours worth of data (36 billion micros, see table below)

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
    \toprule
    date & min timestamp & max timestamp & avg bbo mid & avg 5 level order volume \\
    \midrule
    20190610 & 0 & 36000000000 & 10064 & 673 \\
    20190611 & 0 & 36000000000 & 10127 & 866 \\
    20190612 & 0 & 36000000000 & 9999 & 955 \\
    20190613 & 0 & 36000000000 & 10065 & 908 \\
    20190614 & 0 & 35999621354 & 9894 & 797 \\
    \bottomrule
    \end{tabular}
    \label{tab2}
\end{table}


\subsection{Data resampling}
The order book is of the form below:

\begin{table}[H]
    \centering
    \begin{tabular}{crrrr}
    \toprule
    timestamp & bp0 & bq0 & ap0 & aq0 \\
    \midrule
    110 & 10045& 62 & 10055 & 98 \\
    175 & 10065& 46 & 10075 & 42 \\
    220 & 10075& 9 & 10080 & 25 \\
    \bottomrule
    \end{tabular}
    \label{tab2}
\end{table}

where the timestamps are in microseconds. Plotting a histogram of the frequency of the timestamps,
we see that the updates aren't uniformly distributed: 

 \begin{figure}[H] 
	\centering
	\includegraphics[width=0.90\textwidth]{../data/figures/hist_20190610_timestamps.png}
	\caption{Histogram of order update arrivals for date 20190610}
	\label{fig1}
\end{figure}

For the analysis the data will be resampled. Specifically, a grid will be used with a time interval of
100,000 micros = 100ms. This is done so as to reduce noise of the raw updates at microsecond level and detect any
signals in the data. Obviously, this grid size might cause information loss and a more thorough
analysis can be done to optimize, but for practical considerations 100ms will be used as the
discretization step, since this will allow for quicker data processing and model estimation times. 
In practice this means the timestamps will be rounded up to the nearest 100,000 micros.
The reason to round up is to avoid look-ahead bias when using the data as a trade signal, since
rounding down will match an order update with a timepoint in the past. In addition, when
discretizing to a grid, there might an issue when there are no updates available. In that case a
forward interpolation is done by using the last known value.

\section{Feature Selection}
\subsection{Target to predict}
For the targets the following was considered:
\begin{itemize}
    \item The simple average mid price:
        \begin{equation}
            P_{mid} = \frac{bp0 + ap0}{2} \label{bbomid}
        \end{equation}
    \item The inverse volume weighted mid price:
        \begin{equation}
            P_{mid}^1 = \frac{bp0\times aq0 + ap0 \times bq0}{bq0 + aq0} \label{invmid1}
        \end{equation}
        This mid has the benefit of taking into account the order imbalance at the top level: if
        the buy order volume is higher, the price will be skewed higher to the ask, and vice-versa
        if the sell order volume is higher.
    \item The inverse volume weighted mid price at the first and second level:
        \begin{equation}
            P_{mid}^2 = \frac{bp0\times aq0 + ap0 \times bq0 + bp1\times aq1 + ap1 \times bq1}{bq0 +
            aq0+bq1+aq1} \label{invmid2}
        \end{equation}
        This mid has the same advantage as the previous one and it also takes into account the
        second layer of the orderbook
\end{itemize}

For the target the simple average mid (equation \ref{bbomid}) is used rather than one of the two
inverse weighted mids. The reasoning being that the inverse weighted mids are predictors of sorts for the simple average mid. 

Another interesing target would be the mid price change from time $t$  $t+\delta$ where the $\delta$
is set to a multiple of the discretization step 100ms. The logic being that the price moves is what
the market focueses on, rather than the absolute price level. To keep things simple, the target
variable to be predicted will be the simple average mid price.

Below are the plots of the mid price and iverse weighted mid price for different days. Looking at
the plots there are no weird outliers. Also clearly the inverse weighted mid price closely track the
simple average mid price.
\begin{figure}[H] 
	\centering
	\includegraphics[width=0.70\textwidth]{../data/figures/time_series_20190610_mid_price_inv_mid_price.png}
	\caption{Day 2019-06-10}
	\label{fig2}
\end{figure}

\begin{figure}[H] 
	\centering
	\includegraphics[width=0.70\textwidth]{../data/figures/time_series_20190612_mid_price_inv_mid_price.png}
	\caption{Day 2019-06-12}
	\label{fig3}
\end{figure}


% \begin{figure}[H] 
%	\centering
%	\includegraphics[width=0.70\textwidth]{../data/figures/time_series_20190610_returns_abs.png}
%	\caption{}
%	\label{fig3}
%\end{figure}

\subsection{Features}
For the features, the ones below are considerd:
\begin{itemize}
    \item The bid-offer spread calculated as:
        \begin{equation}
            BOspread = \frac{ap0 - bp0}{P_{mid}}
        \end{equation}
    The intuition of using the spread as a predictor for the change of the mid-price is that if say
    the spread is relatively wide, then the probability of a non-markeatable limit order
    arriving whose price is inside the spread is also higher. Whereas if the bid-offer spread was very
    narrow, then the mid-price can only change when side of the order-book is depleted.
    \item Order imbalance at the top level: 
        \begin{equation}
            OI_0 = \frac{bq0}{bq0+aq0} 
        \end{equation}
    This intuition behind this metric is that if the order queue on the bid side larger than on the
    ask side, the ask side will be depleted sooner and therefore its predictive of an upward
    price move. This ratio is chosen over a simple subration, is because its normalized, which
    reduces bias in the model fitting. Note that if the top level bid size queue is larger than the
        one on the ask side, the ratio will be close to 1. And if the ask side has a much larger
        queue the ratio will be cloe to 0.
    \item Order imbalance at the second level: same as the above metric, but at the second level of
        the order book:
        \begin{equation}
            OI_1 = \frac{bq1}{bq1+aq1} 
        \end{equation}
    \item Change in $bq0$ relative to the previous period:
        \begin{equation}
            \Delta bq0 = bq0_{t} - bq0_{t-\delta} 
        \end{equation}
        The intuition behind this is that the arrival of a large order on the bid side indicates
        more buying pressure and vice versa when a large buy order is removed by a markeatable
        action. Both cases make it likely for the mid price to move.
    \item Change in $aq0$ relative to the previous period:
        \begin{equation}
            \Delta aq0 = aq0_{t} - aq0_{t-\delta} 
        \end{equation}
        The intuition behind this is the same as the one for the bid quantity increasing.
    \item Finally the inverse mid price will also be considered as defined in equation
        (\ref{invmid2})
\end{itemize}



\section{Model Selection}
The model that will be estimated using a Lasso regression:

\begin{equation}
    P_{mid, t + \delta} = BOspread_{t} + OI_{0, t} + OI_{1,t} + \Delta bq_{0,t} + \Delta aq_{0,t} +
    P_{mid, t}^2 + \beta_0 + \epsilon 
\end{equation}

where $\epsilon$ is the error term and $\beta_0$ is the intercept. For ease of notation, the coefficients are omitted in the formula above. 
Note that the inverse weighted mid price of two layers $P_{mid, t}^2$ will be heavily correlated with $P_mid$. 
Hence, the regression will be run twice: with and without the inverse weighted mid price.

\section{Results}
The model above is estimated using a Lasso regression. We try out different alphas and report the
$R^2$ score of both the test and train set. The results are given below:

\begin{table}[H]
  \centering
  \begin{minipage}{.4\textwidth}
    \centering
    \begin{tabular}{lrr}
    \toprule
    date & test score & train score \\
    \midrule
    20190610 & 0.999716 & 0.999710 \\
    20190611 & 0.999766 & 0.999767 \\
    20190612 & 0.999935 & 0.999936 \\
    20190613 & 0.999639 & 0.999640 \\
    20190614 & 0.999900 & 0.999900 \\
    \bottomrule
    \end{tabular}
    \caption{Model with $\alpha = 0.04$}
  \end{minipage}
  \hspace{1cm}
  \begin{minipage}{.4\textwidth}
    \centering
    \begin{tabular}{lrr}
    \toprule
    date & test score & train score \\
    \midrule
    20190610 & 0.996999 & 0.997010 \\
    20190611 & 0.995976 & 0.995977 \\
    20190612 & 0.998965 & 0.998966 \\
    20190613 & 0.993737 & 0.993749 \\
    20190614 & 0.998459 & 0.998463 \\
    \bottomrule
    \end{tabular}
    \caption{Model with $\alpha = 1$}
  \end{minipage}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{lrr}
    \toprule
    date & test score & train score \\
    \midrule
    20190610 & 0.873826 & 0.873907 \\
    20190611 & 0.815287 & 0.815320 \\
    20190612 & 0.952993 & 0.952984 \\
    20190613 & 0.771241 & 0.771078 \\
    20190614 & 0.954335 & 0.954352 \\
    \bottomrule
    \end{tabular}
    \caption{Model with $\alpha = 10$}
\end{table}

Note that as the alpha is increased, the score overall go down which is the effect of the increasing
alpha.

The high scores are mostly due to the inclusion of  $P_{mid, t}^2$. In the following tables, that
the term is omitted:


\begin{table}[H]
  \centering
  \begin{minipage}{.4\textwidth}
    \centering
    \begin{tabular}{lrr}
    \toprule
    date & test score & train score \\
    \midrule
    20190610 & 0.005131 & 0.005719 \\
    20190611 & 0.015000 & 0.014752 \\
    20190612 & 0.003620 & 0.003753 \\
    20190613 & 0.074478 & 0.078335 \\
    20190614 & 0.186116 & 0.187572 \\
    \bottomrule
    \end{tabular}
    \caption{Model with $\alpha = 0.04$}
  \end{minipage}
  \hspace{1cm}
  \begin{minipage}{.4\textwidth}
    \centering
    \begin{tabular}{lrr}
    \toprule
    date & test score & train score \\
    \midrule
    20190610 & 0.003133 & 0.003457 \\
    20190611 & 0.011403 & 0.011313 \\
    20190612 & 0.002394 & 0.002521 \\
    20190613 & 0.068101 & 0.071400 \\
    20190614 & 0.184384 & 0.185805 \\
    \bottomrule
    \end{tabular}
    \caption{Model with $\alpha = 1$}
  \end{minipage}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{lrr}
    \toprule
    date & test score & train score \\
    \midrule
    20190610 & 0.000000 & -0.000000 \\
    20190611 & 0.000000 & -0.000001 \\
    20190612 & 0.000000 & -0.000002 \\
    20190613 & 0.000000 & -0.000012 \\
    20190614 & 0.109777 & 0.110458 \\
    \bottomrule
    \end{tabular}
    \caption{Model with $\alpha = 10$}
\end{table}

The coefficients are:


\begin{table}[H]
  \centering
  \begin{minipage}{.4\textwidth}
    \centering
      \begin{tabular}{lr}
      \toprule
      feature & coefficient \\
      \midrule
      $BOspread$ & 0.237939 \\
      $OI_0$ & 2.080445 \\
      $OI_1$ & -0.701317 \\
      $\Delta bq_{0}$ & -0.092875 \\
      $\Delta aq_{0}$ & 0.117881 \\
      \bottomrule
      \end{tabular}
  \caption{ $\alpha = 0.04$ and 2019-06-10}
  \end{minipage}
  \begin{minipage}{.4\textwidth}
    \centering
  \begin{tabular}{lr}
  \toprule
  feature & coefficient \\
  \midrule
  $BOspread$ & -1.000401 \\
  $OI_0$ & -0.785708 \\
  $OI_1$ & -2.308759 \\
  $\Delta bq_{0}$ & 0.000000 \\
  $\Delta aq_{0}$ & 0.002927 \\
  \bottomrule
  \end{tabular}
  \caption{ $\alpha = 0.04$ and 2019-06-12}
  \end{minipage}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{lr}
    \toprule
    feature & coefficient \\
    \midrule
    $BOspread$ & 4.542120 \\
    $OI_0$ & 13.765068 \\
    $OI_1$ & 10.569343 \\
    $\Delta bq_0$ & -0.987877 \\
    $\Delta aq_0$ & 0.441522 \\
    \bottomrule
    \end{tabular}
    \caption{ $\alpha = 0.04$ and 2019-06-14}
\end{table}
%%\section{Conclusion}       

\bibliographystyle{plain}
%\bibliography{report_sci_comp_3}

\end{document}
